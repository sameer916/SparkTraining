{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 04\n",
    "\n",
    "Weightage: 10\n",
    "\n",
    "Get the number of members per city. There might be duplicate city names which might belong to different states and hence make sure that you include state as well while getting the count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "All of the address data is available under **/public/addresses**. Here is the schema.\n",
    "```\n",
    "root\n",
    " |-- address: struct (nullable = true)\n",
    " |    |-- city: string (nullable = true)\n",
    " |    |-- postal_code: string (nullable = true)\n",
    " |    |-- state: string (nullable = true)\n",
    " |    |-- street: string (nullable = true)\n",
    " |-- email: string (nullable = true)\n",
    " |-- first_name: string (nullable = true)\n",
    " |-- gender: string (nullable = true)\n",
    " |-- id: long (nullable = true)\n",
    " |-- ip_address: string (nullable = true)\n",
    " |-- last_name: string (nullable = true)\n",
    " |-- phone_numbers: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Requirements\n",
    "* Place the result in the HDFS Directory \n",
    "```\n",
    "/user/`whoami`/mock_test_02/problem04/solution\n",
    "```\n",
    "* Use Parquet with Snappy compression. Make sure that data is saved in exactly one file.\n",
    "* Here are the column names. Data types should be same as input data.\n",
    "```\n",
    " |-- state: string\n",
    " |-- city: string\n",
    " |-- member_count: long\n",
    "```\n",
    "* Data should be sorted in ascending order by state and then in descending order by count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Here are the self validation steps:\n",
    "* Run the following command and check for the extension. It should contain snappy and parquet.\n",
    "```\n",
    "hdfs dfs -ls /user/`whoami`/mock_test_02/problem04/solution\n",
    "```\n",
    "* Run the following code to create data frame.\n",
    "```\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "path = f'/user/{username}/mock_test_02/problem04/solution'\n",
    "data = spark. \\\n",
    "    read. \\\n",
    "    parquet(path)\n",
    "```\n",
    "* Get Schema by running `data.printSchema()`. Output should be as below. Ignore Nullability if it does not match exactly.\n",
    "```\n",
    "root\n",
    " |-- state: string (nullable = true)\n",
    " |-- city: string (nullable = true)\n",
    " |-- member_count: long (nullable = true)\n",
    "```\n",
    "* Get count by running `data.count()`. It should return **494**.\n",
    "* Run below code to validate the data.\n",
    "```\n",
    "from pyspark.sql.functions import col\n",
    "data.orderBy(col('state'), col('member_count').desc()).show()\n",
    "```\n",
    "* Sample output\n",
    "\n",
    "|  state|           city|member_count|\n",
    "|-------|---------------|------------|\n",
    "|Alabama|     Birmingham|        7987|\n",
    "|Alabama|     Montgomery|        4279|\n",
    "|Alabama|         Mobile|        4236|\n",
    "|Alabama|     Huntsville|        2139|\n",
    "|Alabama|     Tuscaloosa|        1071|\n",
    "|Alabama|        Gadsden|         500|\n",
    "|Alabama|       Anniston|         499|\n",
    "| Alaska|      Anchorage|        2684|\n",
    "| Alaska|      Fairbanks|        1105|\n",
    "| Alaska|         Juneau|         530|\n",
    "|Arizona|        Phoenix|        8625|\n",
    "|Arizona|         Tucson|        5289|\n",
    "|Arizona|     Scottsdale|        1559|\n",
    "|Arizona|           Mesa|        1551|\n",
    "|Arizona|       Glendale|        1077|\n",
    "|Arizona|        Gilbert|         561|\n",
    "|Arizona|       Chandler|         537|\n",
    "|Arizona|Apache Junction|         527|\n",
    "|Arizona|         Peoria|         516|\n",
    "|Arizona|       Prescott|         512|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName(f'Problem 02 | {username}'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "addressPath=\"/public/addresses/Address-*.json\"\n",
    "addressDf=spark.read.json(addressPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lit,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=addressDf. \\\n",
    "        groupBy(\"address.state\",\"address.city\"). \\\n",
    "        agg(count(lit(1)).alias('member_count')). \\\n",
    "        orderBy('address.state',col('member_count').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.coalesce(1). \\\n",
    "     write.option('compression','snappy'). \\\n",
    "    parquet(f'/user/{username}/mock_test_02/problem04/solution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv001477 supergroup          0 2021-12-10 01:36 /user/itv001477/mock_test_02/problem04/solution/_SUCCESS\n",
      "-rw-r--r--   3 itv001477 supergroup       8140 2021-12-10 01:36 /user/itv001477/mock_test_02/problem04/solution/part-00000-857bb757-aeaa-455c-8d14-ff0e8b889441-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/`whoami`/mock_test_02/problem04/solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'/user/{username}/mock_test_02/problem04/solution'\n",
    "data = spark. \\\n",
    "  read. \\\n",
    "  parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- member_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------+\n",
      "|  state|           city|member_count|\n",
      "+-------+---------------+------------+\n",
      "|Alabama|     Birmingham|        7987|\n",
      "|Alabama|     Montgomery|        4279|\n",
      "|Alabama|         Mobile|        4236|\n",
      "|Alabama|     Huntsville|        2139|\n",
      "|Alabama|     Tuscaloosa|        1071|\n",
      "|Alabama|        Gadsden|         500|\n",
      "|Alabama|       Anniston|         499|\n",
      "| Alaska|      Anchorage|        2684|\n",
      "| Alaska|      Fairbanks|        1105|\n",
      "| Alaska|         Juneau|         530|\n",
      "|Arizona|        Phoenix|        8625|\n",
      "|Arizona|         Tucson|        5289|\n",
      "|Arizona|     Scottsdale|        1559|\n",
      "|Arizona|           Mesa|        1551|\n",
      "|Arizona|       Glendale|        1077|\n",
      "|Arizona|        Gilbert|         561|\n",
      "|Arizona|       Chandler|         537|\n",
      "|Arizona|Apache Junction|         527|\n",
      "|Arizona|         Peoria|         516|\n",
      "|Arizona|       Prescott|         512|\n",
      "+-------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.orderBy(col('state'), col('member_count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
